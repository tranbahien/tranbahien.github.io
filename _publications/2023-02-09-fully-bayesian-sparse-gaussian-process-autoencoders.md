---
title: "Fully Bayesian Autoencoders with Latent Sparse Gaussian Processes"
collection: publications
permalink: /publication/2023-02-09-fully-bayesian-sparse-gaussian-process-autoencoders
excerpt: ''
date: 2023-04-24
venue: 'ICML'
citation: 'Tran, Ba-Hien; Shahbaba, Babak; Mandt, Stephan; Filippone, Maurizio. Fully Bayesian Autoencoders with Latent Sparse Gaussian Processes. <i>International Conference on Machine Learning</i>, 2023.'
---
Autoencoders and their variants are among the most widely used models in representation learning and generative modeling. However, autoencoder-based models usually assume that the learned representations are i.i.d. and fail to capture the correlations between the data samples. To address this issue, we propose a novel Sparse Gaussian Process Bayesian Autoencoder (SGPBAE) model in which we impose fully Bayesian sparse Gaussian Process priors on the latent space of a Bayesian Autoencoder. We perform posterior estimation for this model via stochastic gradient Hamiltonian Monte Carlo. We evaluate our approach qualitatively and quantitatively on a wide range of representation learning and generative modeling tasks and show that our approach consistently outperforms multiple alternatives relying on Variational Autoencoders.

[Download paper here](https://arxiv.org/pdf/2302.04534.pdf)

<!-- Recommended citation: Tran, Ba-Hien et al. (2021). "Functional Priors for bayesian neural networks through wasserstein distance minimization to Gaussian processes." <i>ArXiv</i>. 1(1). -->